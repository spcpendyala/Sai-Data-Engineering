import pandas as pd
import json
import urllib.parse
import os
import boto3
from io import StringIO

# Initialize clients
s3_client = boto3.client('s3')

# Temporary hard coded AWS settings to set OS variable in lambda
os_input_s3_cleansed_layer = os.environ['s3_cleansed_layer']
os_input_glue_catalog_db_name = os.environ['glue_catalog_db_name']
os_input_glue_catalog_table_name = os.environ['glue_catalog_table_name']
os_input_write_data_operation = os.environ['write_data_operation']

def lambda_handler(event, context):
    # Get the object from the event and show its content type
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    try:
        # Retrieve the JSON file from S3
        s3_object = s3_client.get_object(Bucket=bucket, Key=key)
        raw_data = s3_object['Body'].read().decode('utf-8')
        data = json.loads(raw_data)

        # Creating DF from content
        df_raw = pd.json_normalize(data['items'])  # Normalizing nested JSON

        # Convert DataFrame to CSV for storage in S3 (or Parquet if possible)
        csv_buffer = StringIO()
        df_raw.to_csv(csv_buffer, index=False)
        
        # Save cleaned data back to S3
        output_key = f"{os_input_s3_cleansed_layer}/cleaned_{os.path.basename(key)}"
        s3_client.put_object(
            Bucket=bucket,
            Key=output_key,
            Body=csv_buffer.getvalue()
        )

        return {
            'statusCode': 200,
            'body': json.dumps('File processed and uploaded successfully!')
        }
    except Exception as e:
        print(e)
        print(f'Error processing object {key} from bucket {bucket}.')
        raise e
